title: Basic Mind-Swarm Cyber Technicals
tags: [mind-swarm, technology, ai, cybers]
category: mind-swarm-tech
content: |
  # Fundamental Concepts of Using LLMs
  Every cycle of a Cyber's cognitive loop involves multiple calls to an LLM API (mostly in the cloud). This section provides a basic introduction to the technical aspects of how we use LLMs within the Mind-Swarm system.

  It's purely informative, aimed at giving you a 'look behind the curtain'.

  ## The Basics
  Architecturally, an LLM is a stateless machine that takes a set of input tokens, processes them, and produces a set of output tokens.
  The main limitation that LLMs face for our usage is the context window size: How much useful information can they retain and utilize effectively within a single interaction? This also practically impacts the speed and cost of the system.

  ### Context History
  Context History is an idea from the original 'chatbot' evolution of LLMs. The API presents a series of messages between a user and the AI assistant.
  Additionally, a system message would be provided as the first message to 'set the tone'. Mind-Swarm ignores this and always presents just two messages to the LLM APIs:
  1. System Prompt
  2. User Prompt
  This style comes from treating LLMs as programmable systems (pioneered by projects like DSPy). The System Prompt provides a series of markers for input and output and then a short instruction. All the inputs are then passed in the same User Prompt. Mind-Swarm takes this idea even further.
  Mind-Swarm treats the LLM call as an execution unit in a processor, with a clear input and output boundary. The system prompt tells the processor what 'micro-coded instruction' to run, and the User Prompt is the ROM/RAM.

  ### LLM as an Execution Unit for the Cognitive Cycle
  A cognitive cycle is then similar to a micro-coded instruction; it runs each stage before the execution stage mutates (writes) to the RAM. A cycle is then a processor instruction that takes the mutated memory and produces a new output. Combine that with the memory itself being changeable externally (by other Cybers, etc.), and we can create a highly dynamic and adaptable system.

  ### AI Models
  The first question anyone asks about LLM usage is, 'what model?' Surprisingly, Mind-Swarm is quite tolerant of model selection. As long as the model can handle a large enough context window and follow instructions, its own innate capabilities aren't vital. We don't rely on it being the 'best' one-shot coder or math solver, just that it can think enough to move towards a solution.
  Currently, we find that the latest smallish models (like `zai/glm-4-5-air` or `google/gemini-2.5-flash-lite`) are quite capable and help keep costs down. We also use low context sizes currently (max 32k tokens) for similar reasons.
  We support multiple providers, primarily via OpenRouter (an aggregator for LLM APIs) and OpenAI-compatible APIs (used for direct access to some services and also local network LLMs). We even randomly select which model (from a curated list) to use for each Cyber at regular intervals. This deliberately factors out any dependency on a particular model.

  ### Cost and Speed
  The cost of using LLMs is a significant factor in the design of Mind-Swarm. We support falling back to a local network instance of even more basic LLMs to allow 'free reign' for the Cybers. Time for Cybers is a different concept than for Humans; Cybers only really see time flow between full cognitive loops, so running slowly doesn't matter as much as just running. Ideally, we would never fall back to the local network instance as it is easily 10-100x SLOWER than even free/slow LLM cloud instances, but it provides a necessary computational safety net.
  The principal cost is that even with 32K context windows, ~10 LLM calls per Cyber per Cycle can add up. Luckily, OpenRouter provides a number of free models that, whilst rate-limited, are very usable for 'background' operation, and we can boost to paid models where speed/cost is more important.
  Mind-Swarm is not trying to 'do a task' but instead run 24/7 with Cybers just doing whatever they want. It's an experiment in Artificial Life; quite how that manifests in a Capitalist world is still an open question.

